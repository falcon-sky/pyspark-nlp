{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x7fdb0c3b5510>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://192.168.0.5:4041\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Frame: From Panda"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   month  avg_high  avg_low  record_high  record_low  avg_precipitation\n0    JAN        23       15           74          22               2.95\n1    FEB        25       16           74          22               2.95\n2    MAR        28       20           74          22               2.95\n3    APR        29       23           74          22               2.95\n4    MAY        35       27           74          22               2.95\n5    JUN        36       26           74          22               2.95\n6    JUL        34       25           74          22               2.95\n7    AUG        30       24           74          22               2.95\n8    SEP        29       24           74          22               2.95\n9    OCT        26       23           74          22               2.95\n10   NOV        24       22           74          22               2.95\n11   DEC        22       20           74          22               2.95",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>month</th>\n      <th>avg_high</th>\n      <th>avg_low</th>\n      <th>record_high</th>\n      <th>record_low</th>\n      <th>avg_precipitation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>JAN</td>\n      <td>23</td>\n      <td>15</td>\n      <td>74</td>\n      <td>22</td>\n      <td>2.95</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>FEB</td>\n      <td>25</td>\n      <td>16</td>\n      <td>74</td>\n      <td>22</td>\n      <td>2.95</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MAR</td>\n      <td>28</td>\n      <td>20</td>\n      <td>74</td>\n      <td>22</td>\n      <td>2.95</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>APR</td>\n      <td>29</td>\n      <td>23</td>\n      <td>74</td>\n      <td>22</td>\n      <td>2.95</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MAY</td>\n      <td>35</td>\n      <td>27</td>\n      <td>74</td>\n      <td>22</td>\n      <td>2.95</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>JUN</td>\n      <td>36</td>\n      <td>26</td>\n      <td>74</td>\n      <td>22</td>\n      <td>2.95</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>JUL</td>\n      <td>34</td>\n      <td>25</td>\n      <td>74</td>\n      <td>22</td>\n      <td>2.95</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>AUG</td>\n      <td>30</td>\n      <td>24</td>\n      <td>74</td>\n      <td>22</td>\n      <td>2.95</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>SEP</td>\n      <td>29</td>\n      <td>24</td>\n      <td>74</td>\n      <td>22</td>\n      <td>2.95</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>OCT</td>\n      <td>26</td>\n      <td>23</td>\n      <td>74</td>\n      <td>22</td>\n      <td>2.95</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>NOV</td>\n      <td>24</td>\n      <td>22</td>\n      <td>74</td>\n      <td>22</td>\n      <td>2.95</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>DEC</td>\n      <td>22</td>\n      <td>20</td>\n      <td>74</td>\n      <td>22</td>\n      <td>2.95</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_temperature = pd.DataFrame([['JAN', 23, 15, 74, 22, 2.95],\n",
    "                   ['FEB', 25, 16, 74, 22, 2.95],\n",
    "                   ['MAR', 28, 20, 74, 22, 2.95],\n",
    "                   ['APR', 29, 23, 74, 22, 2.95],\n",
    "                   ['MAY', 35, 27, 74, 22, 2.95],\n",
    "                   ['JUN', 36, 26, 74, 22, 2.95],\n",
    "                   ['JUL', 34, 25, 74, 22, 2.95],\n",
    "                   ['AUG', 30, 24, 74, 22, 2.95],\n",
    "                   ['SEP', 29, 24, 74, 22, 2.95],\n",
    "                   ['OCT', 26, 23, 74, 22, 2.95],\n",
    "                   ['NOV', 24, 22, 74, 22, 2.95],\n",
    "                   ['DEC', 22, 20, 74, 22, 2.95]],\n",
    "                  index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "                  columns=['month', 'avg_high', 'avg_low', 'record_high', 'record_low', 'avg_precipitation'])\n",
    "\n",
    "df_temperature"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Frame spark from list of ROW"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: string (nullable = true)\n",
      " |-- avg_high: long (nullable = true)\n",
      " |-- avg_low: long (nullable = true)\n",
      " |-- record_high: long (nullable = true)\n",
      " |-- record_low: long (nullable = true)\n",
      " |-- avg_precipitation: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "df_spark_temperature = spark.createDataFrame([\n",
    "    Row(month='JAN', avg_high=23, avg_low=15, record_high=74, record_low=22, avg_precipitation=2.95),\n",
    "    Row(month='FEB', avg_high=25, avg_low=15, record_high=74, record_low=22, avg_precipitation=2.95),\n",
    "    Row(month='MAR', avg_high=28, avg_low=15, record_high=74, record_low=22, avg_precipitation=2.95),\n",
    "    Row(month='APR', avg_high=23, avg_low=15, record_high=74, record_low=22, avg_precipitation=2.95),\n",
    "    Row(month='MAY', avg_high=23, avg_low=15, record_high=74, record_low=22, avg_precipitation=2.95),\n",
    "    Row(month='JUN', avg_high=23, avg_low=15, record_high=74, record_low=22, avg_precipitation=2.95),\n",
    "    Row(month='JUL', avg_high=23, avg_low=15, record_high=74, record_low=22, avg_precipitation=2.95),\n",
    "    Row(month='AUG', avg_high=23, avg_low=15, record_high=74, record_low=22, avg_precipitation=2.95),\n",
    "    Row(month='SEP', avg_high=23, avg_low=15, record_high=74, record_low=22, avg_precipitation=2.95),\n",
    "    Row(month='OCT', avg_high=23, avg_low=15, record_high=74, record_low=22, avg_precipitation=2.95),\n",
    "    Row(month='NOV', avg_high=23, avg_low=15, record_high=74, record_low=22, avg_precipitation=2.95),\n",
    "    Row(month='DEC', avg_high=23, avg_low=15, record_high=74, record_low=22, avg_precipitation=2.95)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: string (nullable = true)\n",
      " |-- avg_high: long (nullable = true)\n",
      " |-- avg_low: long (nullable = true)\n",
      " |-- record_high: long (nullable = true)\n",
      " |-- record_low: long (nullable = true)\n",
      " |-- avg_precipitation: double (nullable = true)\n",
      "\n",
      "DataFrame[month: string, avg_high: bigint, avg_low: bigint, record_high: bigint, record_low: bigint, avg_precipitation: double]\n"
     ]
    }
   ],
   "source": [
    "df_spark_temperature.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+-----------+----------+-----------------+\n",
      "|month|avg_high|avg_low|record_high|record_low|avg_precipitation|\n",
      "+-----+--------+-------+-----------+----------+-----------------+\n",
      "|  JAN|      23|     15|         74|        22|             2.95|\n",
      "|  FEB|      25|     15|         74|        22|             2.95|\n",
      "|  MAR|      28|     15|         74|        22|             2.95|\n",
      "|  APR|      23|     15|         74|        22|             2.95|\n",
      "|  MAY|      23|     15|         74|        22|             2.95|\n",
      "|  JUN|      23|     15|         74|        22|             2.95|\n",
      "|  JUL|      23|     15|         74|        22|             2.95|\n",
      "|  AUG|      23|     15|         74|        22|             2.95|\n",
      "|  SEP|      23|     15|         74|        22|             2.95|\n",
      "|  OCT|      23|     15|         74|        22|             2.95|\n",
      "|  NOV|      23|     15|         74|        22|             2.95|\n",
      "|  DEC|      23|     15|         74|        22|             2.95|\n",
      "+-----+--------+-------+-----------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_temperature.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a PySpark DataFrame with an explicit schema."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of object (16) does not match with length of fields (6)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-bbbec139fa62>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     12\u001B[0m                     \u001B[0;34m(\u001B[0m\u001B[0;34m'NOV'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m24\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m22\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m74\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m22\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2.95\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m                     ('DEC', 22, 20, 74, 22, 2.95))\n\u001B[0;32m---> 14\u001B[0;31m ], schema='month string, avg_high bigint, avg_low bigint, record_high bigint, record_low bigint, avg_precipitation float')\n\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36mcreateDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    673\u001B[0m             return super(SparkSession, self).createDataFrame(\n\u001B[1;32m    674\u001B[0m                 data, schema, samplingRatio, verifySchema)\n\u001B[0;32m--> 675\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    676\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    677\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    698\u001B[0m             \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    699\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 700\u001B[0;31m             \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromLocal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    701\u001B[0m         \u001B[0mjrdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSerDeUtil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoJavaArray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_to_java_object_rdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    702\u001B[0m         \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapplySchemaToPythonRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    507\u001B[0m         \u001B[0;31m# make sure data could consumed multiple times\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    508\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 509\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    510\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    511\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36mprepare\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m    680\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    681\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mprepare\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 682\u001B[0;31m                 \u001B[0mverify_func\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    683\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    684\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDataType\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/types.py\u001B[0m in \u001B[0;36mverify\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1409\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mverify\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1410\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mverify_nullability\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1411\u001B[0;31m             \u001B[0mverify_value\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1412\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1413\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mverify\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/types.py\u001B[0m in \u001B[0;36mverify_struct\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1388\u001B[0m                     raise ValueError(\n\u001B[1;32m   1389\u001B[0m                         new_msg(\"Length of object (%d) does not match with \"\n\u001B[0;32m-> 1390\u001B[0;31m                                 \"length of fields (%d)\" % (len(obj), len(verifiers))))\n\u001B[0m\u001B[1;32m   1391\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifier\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifiers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1392\u001B[0m                     \u001B[0mverifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Length of object (16) does not match with length of fields (6)"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "                    ('JAN', 23, 15, 74, 22, 2.95),\n",
    "                    ('FEB', 25, 16, 74, 22, 2.95,\n",
    "                    ('MAR', 28, 20, 74, 22, 2.95),\n",
    "                    ('APR', 29, 23, 74, 22, 2.95),\n",
    "                    ('MAY', 35, 27, 74, 22, 2.95),\n",
    "                    ('JUN', 36, 26, 74, 22, 2.95),\n",
    "                    ('JUL', 34, 25, 74, 22, 2.95),\n",
    "                    ('AUG', 30, 24, 74, 22, 2.95),\n",
    "                    ('SEP', 29, 24, 74, 22, 2.95),\n",
    "                    ('OCT', 26, 23, 74, 22, 2.95),\n",
    "                    ('NOV', 24, 22, 74, 22, 2.95),\n",
    "                    ('DEC', 22, 20, 74, 22, 2.95))\n",
    "], schema='month string, avg_high bigint, avg_low bigint, record_high bigint, record_low bigint, avg_precipitation float')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-7bd40b95",
   "language": "python",
   "display_name": "PyCharm (pyspark-nlp)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}