{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.5:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f068ce94e10>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='182 86 M'),\n",
       " Row(value='193 112 M'),\n",
       " Row(value='172 72 M'),\n",
       " Row(value='170 61 F')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading from text file\n",
    "\n",
    "df_text = spark.read.text(\"data/bodies.txt\")\n",
    "\n",
    "df_text.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='sepal_length', _c1='sepal_width', _c2='petal_length', _c3='petal_width', _c4='species'),\n",
       " Row(_c0='5.1', _c1='3.5', _c2='1.4', _c3='0.2', _c4='setosa'),\n",
       " Row(_c0='4.9', _c1='3', _c2='1.4', _c3='0.2', _c4='setosa'),\n",
       " Row(_c0='4.7', _c1='3.2', _c2='1.3', _c3='0.2', _c4='setosa')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading from CSV file\n",
    "\n",
    "df_csv = spark.read.csv(\"data/iris.csv\")\n",
    "\n",
    "df_csv.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(SUBDIVISION='Andaman & Nicobar Islands', YEAR=1901, JAN='49.2', FEB='87.1', MAR='29.2', APR='2.3', MAY='528.8', JUN='517.5', JUL='365.1', AUG='481.1', SEP='332.6', OCT='388.5', NOV='558.2', DEC='33.6', ANNUAL='3373.2', JF='136.3', MAM='560.3', JJAS='1696.3', OND='980.3'),\n",
       " Row(SUBDIVISION='Andaman & Nicobar Islands', YEAR=1902, JAN='0', FEB='159.8', MAR='12.2', APR='0', MAY='446.1', JUN='537.1', JUL='228.9', AUG='753.7', SEP='666.2', OCT='197.2', NOV='359', DEC='160.5', ANNUAL='3520.7', JF='159.8', MAM='458.3', JJAS='2185.9', OND='716.7'),\n",
       " Row(SUBDIVISION='Andaman & Nicobar Islands', YEAR=1903, JAN='12.7', FEB='144', MAR='0', APR='1', MAY='235.1', JUN='479.9', JUL='728.4', AUG='326.7', SEP='339', OCT='181.2', NOV='284.4', DEC='225', ANNUAL='2957.4', JF='156.7', MAM='236.1', JJAS='1874', OND='690.6'),\n",
       " Row(SUBDIVISION='Andaman & Nicobar Islands', YEAR=1904, JAN='9.4', FEB='14.7', MAR='0', APR='202.4', MAY='304.5', JUN='495.1', JUL='502', AUG='160.1', SEP='820.4', OCT='222.2', NOV='308.7', DEC='40.1', ANNUAL='3079.6', JF='24.1', MAM='506.9', JJAS='1977.6', OND='571'),\n",
       " Row(SUBDIVISION='Andaman & Nicobar Islands', YEAR=1905, JAN='1.3', FEB='0', MAR='3.3', APR='26.9', MAY='279.5', JUN='628.7', JUL='368.7', AUG='330.5', SEP='297', OCT='260.7', NOV='25.4', DEC='344.7', ANNUAL='2566.7', JF='1.3', MAM='309.7', JJAS='1624.9', OND='630.8')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading From URL\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/ramashanker/dataset/master/climate/India/India_Sub_Division_IMD_2017.csv'\n",
    "from pyspark import SparkFiles\n",
    "spark.sparkContext.addFile(url)\n",
    "df_url = spark.read.csv(\"file://\"+SparkFiles.get(\"India_Sub_Division_IMD_2017.csv\"), header=True, inferSchema= True)\n",
    "df_url.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Generate Data\n",
    "gender_category = ['M', 'F']\n",
    "gender_probs = [0.4, 0.6]\n",
    "def generate_dataset():\n",
    "    gender = np.random.choice(gender_category, p=gender_probs)\n",
    "    if gender == 'M':\n",
    "        height= np.random.normal(loc=140, scale=15, size=1);\n",
    "        weight = np.random.normal(loc=90, scale=10, size=1);\n",
    "        gender = 'Male'\n",
    "    else:\n",
    "        height = np.random.normal(loc=195, scale=10, size=1);\n",
    "        weight = np.random.normal(loc=60, scale=5, size=1);\n",
    "        gender = 'Female'\n",
    "    return int(height[0]),int(weight[0]),gender\n",
    "\n",
    "samples = pd.DataFrame([generate_dataset() for _ in range(50)],\n",
    "                  columns=['height','weight','gender'])\n",
    "samples.to_csv(\"data/population.csv\")\n",
    "spark_sample = spark.createDataFrame(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Write dataframe to csv by default its a part\n",
    "#spark_sample.write.csv('data/part/Normalosa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write a Single file using Spark coalesce() & repartition()\n",
    "#spark_sample.coalesce(1).write.csv(\"data/Normalosa.csv\")\n",
    "#spark_sample.repartition(1).write.csv(\"data/Normalosa.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|,height,weight,ge...|\n",
      "|       0,124,71,Male|\n",
      "|     1,198,53,Female|\n",
      "|     2,197,57,Female|\n",
      "|     3,188,61,Female|\n",
      "|       4,134,88,Male|\n",
      "|       5,112,85,Male|\n",
      "|     6,197,64,Female|\n",
      "|       7,130,94,Male|\n",
      "|     8,203,56,Female|\n",
      "|       9,139,85,Male|\n",
      "|    10,197,59,Female|\n",
      "|    11,196,55,Female|\n",
      "|      12,128,90,Male|\n",
      "|      13,113,64,Male|\n",
      "|    14,197,57,Female|\n",
      "|      15,139,98,Male|\n",
      "|    16,184,49,Female|\n",
      "|      17,166,91,Male|\n",
      "|    18,205,62,Female|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_population = spark.read.text(\"data/population.csv\")\n",
    "df_population.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "data=[(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)]\n",
    "inputRDD = spark.sparkContext.parallelize(data)\n",
    "listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# reduce\n",
    "#Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "720"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "from operator import mul\n",
    "listRdd.reduce(add)\n",
    "listRdd.reduce(mul)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Collect\n",
    "#Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 2, 3, 4, 5, 3, 2]"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listRdd.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# count:Return the number of elements in the dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "7"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listRdd.count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# first: Return the first element of the dataset (similar to take(1))."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listRdd.first()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# take(n):Return an array with the first n elements of the dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 2, 3]"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listRdd.take(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# takeSample(withReplacement, num, [seed]):Return an array with a random sample of num elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#takeOrdered(n, [ordering]): Return the first n elements of the RDD using either their natural order or a custom comparator."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# saveAsTextFile(path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "countByKey()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'parallelize'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-92-548019d2e592>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mrdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"a\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m\"b\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m\"a\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"b\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0msorted\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcountByKey\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'a'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m'b'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'SparkSession' object has no attribute 'parallelize'"
     ]
    }
   ],
   "source": [
    "rdd = spark.parallelize([(\"a\", 2), (\"b\", 1), (\"a\", 1),(\"b\", 3)])\n",
    "sorted(rdd.countByKey().items())\n",
    "[('a', 2), ('b', 1)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "foreach(func)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (pyspark-nlp)",
   "language": "python",
   "name": "pycharm-7bd40b95"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}